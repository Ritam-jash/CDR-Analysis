{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8830a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# CDR Analysis using PySpark\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates how to analyze Call Detail Records (CDR) using PySpark.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Setup and Data Loading\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import sys\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add the parent directory to the path\\n\",\n",
    "    \"sys.path.insert(0, '..')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import project modules\\n\",\n",
    "    \"from utils.spark_session import create_spark_session\\n\",\n",
    "    \"from scripts.read_data import read_cdr_data, define_schema\\n\",\n",
    "    \"from scripts.preprocess import preprocess_cdr_data, validate_data_quality\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create Spark session\\n\",\n",
    "    \"spark = create_spark_session(\\\"CDR Analysis Notebook\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Check if data exists, if not generate it\\n\",\n",
    "    \"if not os.path.exists(\\\"../data/call_logs.csv\\\"):\\n\",\n",
    "    \"    print(\\\"Generating sample data...\\\")\\n\",\n",
    "    \"    from scripts.data_generator import generate_sample_cdr_data\\n\",\n",
    "    \"    generate_sample_cdr_data(\\\"../data/call_logs.csv\\\", num_records=10000)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Read the data\\n\",\n",
    "    \"raw_df = read_cdr_data(spark, \\\"../data/call_logs.csv\\\")\\n\",\n",
    "    \"raw_df.printSchema()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Display sample data\\n\",\n",
    "    \"raw_df.show(5)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Data Preprocessing\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Preprocess the data\\n\",\n",
    "    \"df = preprocess_cdr_data(raw_df)\\n\",\n",
    "    \"df.printSchema()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Validate data quality\\n\",\n",
    "    \"validate_data_quality(df)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cache the data for better performance\\n\",\n",
    "    \"df.cache()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create a temporary view for SQL queries\\n\",\n",
    "    \"df.createOrReplaceTempView(\\\"cdr_data\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Data Exploration\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Get basic statistics\\n\",\n",
    "    \"from pyspark.sql.functions import count, countDistinct\\n\",\n",
    "    \"\\n\",\n",
    "    \"total_records = df.count()\\n\",\n",
    "    \"unique_callers = df.select(countDistinct(\\\"caller_number\\\")).collect()[0][0]\\n\",\n",
    "    \"unique_callees = df.select(countDistinct(\\\"callee_number\\\")).collect()[0][0]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Total records: {total_records}\\\")\\n\",\n",
    "    \"print(f\\\"Unique callers: {unique_callers}\\\")\\n\",\n",
    "    \"print(f\\\"Unique callees: {unique_callees}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Call type distribution\\n\",\n",
    "    \"call_type_dist = df.groupBy(\\\"call_type\\\").count().toPandas()\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"plt.bar(call_type_dist[\\\"call_type\\\"], call_type_dist[\\\"count\\\"])\\n\",\n",
    "    \"plt.title(\\\"Call Type Distribution\\\")\\n\",\n",
    "    \"plt.xlabel(\\\"Call Type\\\")\\n\",\n",
    "    \"plt.ylabel(\\\"Count\\\")\\n\",\n",
    "    \"plt.grid(axis=\\\"y\\\", linestyle=\\\"--\\\", alpha=0.7)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Call status distribution\\n\",\n",
    "    \"status_dist = df.groupBy(\\\"status\\\").count().toPandas()\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"plt.pie(status_dist[\\\"count\\\"], labels=status_dist[\\\"status\\\"], autopct=\\\"%1.1f%%\\\", startangle=90)\\n\",\n",
    "    \"plt.title(\\\"Call Status Distribution\\\")\\n\",\n",
    "    \"plt.axis(\\\"equal\\\")  # Equal aspect ratio ensures that pie is drawn as a circle\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. User Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from pyspark.sql.functions import sum as spark_sum, avg, desc\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Group by caller and calculate metrics\\n\",\n",
    "    \"user_activity = (df.groupBy(\\\"caller_number\\\")\\n\",\n",
    "    \"                 .agg(\\n\",\n",
    "    \"                     count(\\\"*\\\").alias(\\\"total_calls\\\"),\\n\",\n",
    "    \"                     spark_sum(\\\"effective_duration\\\").alias(\\\"total_duration_seconds\\\"),\\n\",\n",
    "    \"                     avg(\\\"effective_duration\\\").alias(\\\"avg_duration_seconds\\\"),\\n\",\n",
    "    \"                     spark_sum(\\\"call_success\\\").alias(\\\"successful_calls\\\"),\\n\",\n",
    "    \"                     spark_sum(\\\"cost\\\").alias(\\\"total_cost\\\")\\n\",\n",
    "    \"                 )\\n\",\n",
    "    \"                 .withColumn(\\\"call_success_rate\\\", \\n\",\n",
    "    \"                             col(\\\"successful_calls\\\") / col(\\\"total_calls\\\")))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show top users by call volume\\n\",\n",
    "    \"user_activity.orderBy(desc(\\\"total_calls\\\")).show(10)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Convert to Pandas for visualization\\n\",\n",
    "    \"top_users = user_activity.orderBy(desc(\\\"total_calls\\\")).limit(10).toPandas()\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"plt.bar(top_users[\\\"caller_number\\\"].astype(str), top_users[\\\"total_calls\\\"])\\n\",\n",
    "    \"plt.title(\\\"Top 10 Users by Call Volume\\\")\\n\",\n",
    "    \"plt.xlabel(\\\"Caller Number\\\")\\n\",\n",
    "    \"plt.ylabel(\\\"Total Calls\\\")\\n\",\n",
    "    \"plt.xticks(rotation=45)\\n\",\n",
    "    \"plt.grid(axis=\\\"y\\\", linestyle=\\\"--\\\", alpha=0.7)\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Time-based Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Hourly patterns\\n\",\n",
    "    \"hourly_patterns = (df.groupBy(\\\"call_hour\\\")\\n\",\n",
    "    \"                  .agg(\\n\",\n",
    "    \"                      count(\\\"*\\\").alias(\\\"total_calls\\\"),\\n\",\n",
    "    \"                      spark_sum(\\\"effective_duration\\\").alias(\\\"total_duration_seconds\\\")\\n\",\n",
    "    \"                  )\\n\",\n",
    "    \"                  .orderBy(\\\"call_hour\\\"))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Convert to Pandas for visualization\\n\",\n",
    "    \"hourly_df = hourly_patterns.toPandas()\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"plt.bar(hourly_df[\\\"call_hour\\\"], hourly_df[\\\"total_calls\\\"])\\n\",\n",
    "    \"plt.title(\\\"Call Volume by Hour of Day\\\")\\n\",\n",
    "    \"plt.xlabel(\\\"Hour of Day\\\")\\n\",\n",
    "    \"plt.ylabel(\\\"Total Calls\\\")\\n\",\n",
    "    \"plt.xticks(range(0, 24))\\n\",\n",
    "    \"plt.grid(axis=\\\"y\\\", linestyle=\\\"--\\\", alpha=0.7)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Operator Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Operator performance\\n\",\n",
    "    \"operator_perf = (df.groupBy(\\\"caller_operator\\\")\\n\",\n",
    "    \"                .agg(\\n\",\n",
    "    \"                    count(\\\"*\\\").alias(\\\"total_calls\\\"),\\n\",\n",
    "    \"                    spark_sum(\\\"call_success\\\").alias(\\\"successful_calls\\\")\\n\",\n",
    "    \"                )\\n\",\n",
    "    \"                .withColumn(\\\"success_rate\\\", col(\\\"successful_calls\\\") / col(\\\"total_calls\\\")))\\n\",\n",
    "    \"\\n\",\n",
    "    \"operator_perf.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Convert to Pandas for visualization\\n\",\n",
    "    \"op_df = operator_perf.toPandas()\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"plt.bar(op_df[\\\"caller_operator\\\"], op_df[\\\"success_rate\\\"] * 100)\\n\",\n",
    "    \"plt.title(\\\"Call Success Rate by Operator\\\")\\n\",\n",
    "    \"plt.xlabel(\\\"Operator\\\")\\n\",\n",
    "    \"plt.ylabel(\\\"Success Rate (%)\\\")\\n\",\n",
    "    \"plt.ylim(0, 100)\\n\",\n",
    "    \"plt.grid(axis=\\\"y\\\", linestyle=\\\"--\\\", alpha=0.7)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. SQL Queries\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Example SQL query: Top 5 callers by total cost\\n\",\n",
    "    \"query = \\\"\\\"\\\"\\n\",\n",
    "    \"SELECT \\n\",\n",
    "    \"    caller_number,\\n\",\n",
    "    \"    SUM(cost) as total_cost,\\n\",\n",
    "    \"    COUNT(*) as total_calls,\\n\",\n",
    "    \"    SUM(cost) / COUNT(*) as avg_cost_per_call\\n\",\n",
    "    \"FROM cdr_data\\n\",\n",
    "    \"GROUP BY caller_number\\n\",\n",
    "    \"ORDER BY total_cost DESC\\n\",\n",
    "    \"LIMIT 5\\n\",\n",
    "    \"\\\"\\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"spark.sql(query).show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Example SQL query: Call duration distribution by call type\\n\",\n",
    "    \"query = \\\"\\\"\\\"\\n\",\n",
    "    \"SELECT \\n\",\n",
    "    \"    call_type,\\n\",\n",
    "    \"    COUNT(*) as total_calls,\\n\",\n",
    "    \"    AVG(duration) as avg_duration,\\n\",\n",
    "    \"    MIN(duration) as min_duration,\\n\",\n",
    "    \"    MAX(duration) as max_duration\\n\",\n",
    "    \"FROM cdr_data\\n\",\n",
    "    \"GROUP BY call_type\\n\",\n",
    "    \"\\\"\\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"spark.sql(query).show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Advanced Analysis: Call Network Graph (Optional)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create a network of callers and callees\\n\",\n",
    "    \"# This is just a preview, full implementation would require NetworkX or similar library\\n\",\n",
    "    \"call_network = df.select(\\\"caller_number\\\", \\\"callee_number\\\", \\\"duration\\\").limit(100).toPandas()\\n\",\n",
    "    \"call_network.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 9. Cleanup\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Clean up Spark session\\n\",\n",
    "    \"spark.stop()\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.5\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
